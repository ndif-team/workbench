remote = true

[models]

[models.one]
name = "openai-community/gpt2"
chat = false
gated = false

[models.one.rename]
transformer = "model"
h = "layers"
c_proj = "o_proj"

[models.one.config]
n_heads = 12
n_layers = 12
params = "124M"

# [models.two]
# name = "Qwen/Qwen3-0.6B-Base"
# chat = false

# [models.two.rename]
# embed_out = "lm_head"
# norm = "ln_f"

# [models.two.config]
# n_heads = 16
# n_layers = 28

[models.two]
name = "EleutherAI/gpt-j-6b"
chat = false
gated = false

[models.two.rename]
transformer = "model"
h = "layers"
out_proj = "o_proj"

[models.two.config]
n_heads = 16
n_layers = 28
params = "6B"

[models.three]
name = "meta-llama/Llama-3.1-8B"
chat = false
gated = true

[models.three.rename]
norm = "ln_f"
self_attn = "attn"

[models.three.config]
n_heads = 32
n_layers = 32
params = "8B"

[models.four]
name = "meta-llama/Llama-3.1-70B"
chat = false
gated = true

[models.four.rename]
norm = "ln_f"
self_attn = "attn"

[models.four.config]
n_heads = 64
n_layers = 80
params = "71B"

[models.five]
name = "meta-llama/Llama-3.1-70B-Instruct"
chat = true
gated = true

[models.five.rename]
norm = "ln_f"
self_attn = "attn"

[models.five.config]
n_heads = 64
n_layers = 80
params = "71B"

#[models.five]
#name = "meta-llama/Llama-3.2-1B-Instruct"
#chat = true
#gated = true

#[models.five.rename]
#norm = "ln_f"
#self_attn = "attn"

#[models.five.config]
#n_heads = 32
#n_layers = 16

[models.six]
name = "meta-llama/Llama-3.3-70B-Instruct"
chat = true
gated = true

[models.six.rename]
norm = "ln_f"
self_attn = "attn"

[models.six.config]
n_heads = 64
n_layers = 80
params = "71B"

[models.seven]
name = "meta-llama/Llama-3.1-405B-Instruct"
chat = true
gated = true

[models.seven.rename]
norm = "ln_f"
self_attn = "attn"

[models.seven.config]
n_heads = 128
n_layers = 126
params = "406B"

[models.eight]
name = "meta-llama/Llama-3.1-405B"
chat = false
gated = true

[models.eight.rename]
norm = "ln_f"
self_attn = "attn"

[models.eight.config]
n_heads = 128
n_layers = 126
params = "406B"

[models.nine]
name = "meta-llama/Llama-2-7b-hf"
chat = false
gated = true

[models.nine.rename]
norm = "ln_f"
self_attn = "attn"

[models.nine.config]
n_heads = 32
n_layers = 32
params = "7B"
